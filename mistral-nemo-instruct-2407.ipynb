{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "776a31b6-d92b-4746-b59c-5e43ed79e7b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T19:20:36.902579Z",
     "iopub.status.busy": "2024-12-15T19:20:36.901988Z",
     "iopub.status.idle": "2024-12-15T19:21:17.710034Z",
     "shell.execute_reply": "2024-12-15T19:21:17.709151Z",
     "shell.execute_reply.started": "2024-12-15T19:20:36.902546Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.46.3\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.3) (3.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.3) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.3) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.3) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.3) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.3) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.3) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.3) (0.20.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.3) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.3) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.3) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.3) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.46.3) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.3) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.3) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.3) (2024.8.30)\n",
      "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.45.1\n",
      "    Uninstalling transformers-4.45.1:\n",
      "      Successfully uninstalled transformers-4.45.1\n",
      "Successfully installed transformers-4.46.3\n",
      "Collecting accelerate==1.1.1\n",
      "  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==1.1.1) (0.25.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==1.1.1) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==1.1.1) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==1.1.1) (5.9.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==1.1.1) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate==1.1.1) (0.4.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==1.1.1) (2.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==1.1.1) (3.15.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==1.1.1) (2024.6.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==1.1.1) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==1.1.1) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==1.1.1) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==1.1.1) (3.1.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==1.1.1) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==1.1.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==1.1.1) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==1.1.1) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==1.1.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==1.1.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==1.1.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==1.1.1) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==1.1.1) (1.3.0)\n",
      "Downloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.34.2\n",
      "    Uninstalling accelerate-0.34.2:\n",
      "      Successfully uninstalled accelerate-0.34.2\n",
      "Successfully installed accelerate-1.1.1\n",
      "Collecting bitsandbytes==0.44.1\n",
      "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.44.1) (2.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.44.1) (1.26.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.44.1) (3.15.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.44.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.44.1) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.44.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.44.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.44.1) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes==0.44.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes==0.44.1) (1.3.0)\n",
      "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.44.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.46.3 \n",
    "!pip install accelerate==1.1.1 \n",
    "!pip install bitsandbytes==0.44.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf50f431-e587-4c96-9652-d10cc951bad6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T19:21:17.711981Z",
     "iopub.status.busy": "2024-12-15T19:21:17.711709Z",
     "iopub.status.idle": "2024-12-15T19:21:17.716002Z",
     "shell.execute_reply": "2024-12-15T19:21:17.715189Z",
     "shell.execute_reply.started": "2024-12-15T19:21:17.711955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cfa8375-6d47-40d1-b64b-7c65baead0f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T19:21:17.717803Z",
     "iopub.status.busy": "2024-12-15T19:21:17.717427Z",
     "iopub.status.idle": "2024-12-15T19:21:17.730614Z",
     "shell.execute_reply": "2024-12-15T19:21:17.729949Z",
     "shell.execute_reply.started": "2024-12-15T19:21:17.717752Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mistral-nemo-instruct-2407\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/input/mistral-nemo-instruct-2407"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20385709-d939-48ea-a604-5313bb20f9b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T19:21:17.732324Z",
     "iopub.status.busy": "2024-12-15T19:21:17.732089Z",
     "iopub.status.idle": "2024-12-15T19:21:18.292880Z",
     "shell.execute_reply": "2024-12-15T19:21:18.291999Z",
     "shell.execute_reply.started": "2024-12-15T19:21:17.732300Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa1c857-919d-4cf0-8a7f-648e11e0b582",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T19:21:18.294805Z",
     "iopub.status.busy": "2024-12-15T19:21:18.294169Z",
     "iopub.status.idle": "2024-12-15T19:21:22.058268Z",
     "shell.execute_reply": "2024-12-15T19:21:22.057554Z",
     "shell.execute_reply.started": "2024-12-15T19:21:18.294750Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a28cc11c-72a7-4a23-9711-c831d36632e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T19:23:27.106485Z",
     "iopub.status.busy": "2024-12-15T19:23:27.105975Z",
     "iopub.status.idle": "2024-12-15T19:23:27.113576Z",
     "shell.execute_reply": "2024-12-15T19:23:27.112258Z",
     "shell.execute_reply.started": "2024-12-15T19:23:27.106454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47a7b914-5362-4481-a1ba-b19d2ea13a93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T19:23:33.281328Z",
     "iopub.status.busy": "2024-12-15T19:23:33.280679Z",
     "iopub.status.idle": "2024-12-15T19:23:33.315462Z",
     "shell.execute_reply": "2024-12-15T19:23:33.314526Z",
     "shell.execute_reply.started": "2024-12-15T19:23:33.281294Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5f1cb30-fb72-4172-a376-fc36ab7daf5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T19:23:33.926832Z",
     "iopub.status.busy": "2024-12-15T19:23:33.926470Z",
     "iopub.status.idle": "2024-12-15T19:26:22.949697Z",
     "shell.execute_reply": "2024-12-15T19:26:22.948776Z",
     "shell.execute_reply.started": "2024-12-15T19:23:33.926804Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a6849ba39544b886513be194550732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"./\", torch_dtype=torch.bfloat16,quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9e8a337-6e33-4695-b4b2-e5f55fbbba28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T19:27:24.892698Z",
     "iopub.status.busy": "2024-12-15T19:27:24.892057Z",
     "iopub.status.idle": "2024-12-15T19:27:25.611736Z",
     "shell.execute_reply": "2024-12-15T19:27:25.611013Z",
     "shell.execute_reply.started": "2024-12-15T19:27:24.892660Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "842147fe-c97e-4a3a-b449-a49d749a3971",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T19:27:25.613286Z",
     "iopub.status.busy": "2024-12-15T19:27:25.613011Z",
     "iopub.status.idle": "2024-12-15T19:27:25.617110Z",
     "shell.execute_reply": "2024-12-15T19:27:25.616274Z",
     "shell.execute_reply.started": "2024-12-15T19:27:25.613260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063c10e4-c45e-44f4-86ed-dcbef867bea0",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d38aaed-ed6d-41c3-a87d-bc0da26b04cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T19:27:26.173745Z",
     "iopub.status.busy": "2024-12-15T19:27:26.173392Z",
     "iopub.status.idle": "2024-12-15T19:27:26.182834Z",
     "shell.execute_reply": "2024-12-15T19:27:26.181968Z",
     "shell.execute_reply.started": "2024-12-15T19:27:26.173715Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from functions_to_call import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a731f281-afd2-4172-86f3-783854523239",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T19:38:17.441578Z",
     "iopub.status.busy": "2024-12-15T19:38:17.440898Z",
     "iopub.status.idle": "2024-12-15T19:38:27.719228Z",
     "shell.execute_reply": "2024-12-15T19:38:27.718346Z",
     "shell.execute_reply.started": "2024-12-15T19:38:17.441547Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.13 s, sys: 1.15 s, total: 10.3 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt=\"give me a satellite snapshot from my location 34.04155284331105, -4.996594567903626\"\n",
    "#prompt=\"who are you\"\n",
    "conversation = [{\"role\": \"user\", \"content\": prompt}]\n",
    "tools = [get_current_weather,get_closest_hospital,get_safest_routes,check_road_closures,find_emergency_supplies,haversine_distance]\n",
    "\n",
    "# format and tokenize the tool use prompt \n",
    "inputs = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tools=tools,\n",
    "            add_generation_prompt=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "inputs.to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=1000)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93b9fdb7-3e71-48a4-af2d-daa99e62904b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T19:36:45.026602Z",
     "iopub.status.busy": "2024-12-15T19:36:45.026243Z",
     "iopub.status.idle": "2024-12-15T19:36:45.033515Z",
     "shell.execute_reply": "2024-12-15T19:36:45.032699Z",
     "shell.execute_reply.started": "2024-12-15T19:36:45.026572Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"34.04155284331105, -4.996594567903626\", \"format\": \"celsius\"}}]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0][len(inputs[\"input_ids\"][0]):],skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ced134b5-7d67-44db-805d-572e5d9b00ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T16:15:00.486694Z",
     "iopub.status.busy": "2024-12-07T16:15:00.486478Z",
     "iopub.status.idle": "2024-12-07T16:15:00.496711Z",
     "shell.execute_reply": "2024-12-07T16:15:00.495883Z",
     "shell.execute_reply.started": "2024-12-07T16:15:00.486672Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "function_map = {\n",
    "    \"get_current_weather\": get_current_weather,\n",
    "    \"get_closest_hospital\":get_closest_hospital,\n",
    "    \"get_safest_routes\":get_safest_routes,\n",
    "    \"check_road_closures\":check_road_closures,\n",
    "    \"find_emergency_supplies\":find_emergency_supplies,\n",
    "    \"haversine_distance\":haversine_distance\n",
    "}\n",
    "\n",
    "def exec_called_func(response,function_map):\n",
    "    match = re.search(r'\\[{\"name\": \"(.*?)\", \"arguments\": (.*?)\\}]', response)\n",
    "    if match:\n",
    "        function_call_part = match.group()  \n",
    "        \n",
    "        try:\n",
    "            function_call = json.loads(function_call_part)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "            function_call = None\n",
    "        \n",
    "        if function_call:\n",
    "            function_name = function_call[0][\"name\"]\n",
    "            arguments = function_call[0][\"arguments\"]\n",
    "            \n",
    "    \n",
    "            if function_name in function_map:\n",
    "                try:\n",
    "                    result = function_map[function_name](**arguments)\n",
    "                    return result\n",
    "                except Exception as e:\n",
    "                    print(f\"Error executing function '{function_name}': {e}\")\n",
    "            else:\n",
    "                print(f\"Unknown function: {function_name}\")\n",
    "    else:\n",
    "        print(\"No function call found in the response.\")\n",
    "#exec_called_func(response,function_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e678ba7-4167-4d00-821d-5cbd6ac14891",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:35:24.485358Z",
     "iopub.status.busy": "2024-12-07T18:35:24.484991Z",
     "iopub.status.idle": "2024-12-07T18:35:24.495073Z",
     "shell.execute_reply": "2024-12-07T18:35:24.494076Z",
     "shell.execute_reply.started": "2024-12-07T18:35:24.485328Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "disaster_info={\n",
    "    \"disaster_report\": {\n",
    "        \n",
    "        \"type\": \"earthquake\",\n",
    "        \"location\": {\n",
    "            \"latitude\": 34.0522,\n",
    "            \"longitude\": -118.2437,\n",
    "            \"city\": \"Los Angeles\",\n",
    "            \"region\": \"California\",\n",
    "            \"country\": \"United States\"\n",
    "        },\n",
    "        \"image_url\":\"https://images.axios.com/-0E5vD5wfUhUCvjCozznsl4ZJFw=/2023/09/11/1694467770528.jpg\",\n",
    "        \"timestamp\": \"2024-11-27T14:30:00Z\",\n",
    "        \"damage_levels\": {\n",
    "            \"infrastructure\": {\n",
    "                \"level\": \"severe\",\n",
    "                \"description\": \"Significant structural damage to buildings and critical infrastructure\",\n",
    "                \"percentage_affected\": 45.5\n",
    "            },\n",
    "            \"residential_areas\": {\n",
    "                \"level\": \"high\",\n",
    "                \"description\": \"Extensive damage to residential structures\",\n",
    "                \"buildings_destroyed\": 1200,\n",
    "                \"buildings_damaged\": 3500\n",
    "            },\n",
    "            \"transportation\": {\n",
    "                \"level\": \"moderate\",\n",
    "                \"description\": \"Road and bridge damage limiting mobility\",\n",
    "                \"roads_blocked\": 37,\n",
    "                \"bridges_compromised\": 12\n",
    "            }\n",
    "        },\n",
    "        \"impact_metrics\": {\n",
    "            \"casualties\": {\n",
    "                \"fatalities\": 87,\n",
    "                \"injured\": 350,\n",
    "                \"displaced\": 5600\n",
    "            },        \n",
    "\n",
    "    }\n",
    "}\n",
    "}\n",
    "\n",
    "\n",
    "def create_system_prompt(disaster_info, user_location):\n",
    "    # Extract necessary information from disaster_info\n",
    "    disaster_type = disaster_info[\"disaster_report\"][\"type\"]\n",
    "    disaster_location = disaster_info[\"disaster_report\"][\"location\"]\n",
    "    disaster_lat = disaster_location[\"latitude\"]\n",
    "    disaster_lon = disaster_location[\"longitude\"]\n",
    "    city = disaster_location[\"city\"]\n",
    "    region = disaster_location[\"region\"]\n",
    "    country = disaster_location[\"country\"]\n",
    "    casualties = disaster_info[\"disaster_report\"][\"impact_metrics\"][\"casualties\"]\n",
    "    fatalities = casualties[\"fatalities\"]\n",
    "    injured = casualties[\"injured\"]\n",
    "    displaced = casualties[\"displaced\"]\n",
    "    disaster_img=disaster_info[\"disaster_report\"][\"image_url\"]\n",
    "    \n",
    "    # Ensure user_location is in the correct format\n",
    "    if isinstance(user_location, dict) and 'latitude' in user_location and 'longitude' in user_location:\n",
    "        user_lat = user_location['latitude']\n",
    "        user_lon = user_location['longitude']\n",
    "        user_loc_str = f\"User Location - {user_lat}, {user_lon}\"\n",
    "    elif isinstance(user_location, str):\n",
    "        user_loc_str = f\"User Location - {user_location}\"\n",
    "    else:\n",
    "        user_loc_str = \"User Location - Unknown\"\n",
    "    \n",
    "    # Construct the system prompt with both disaster and user locations\n",
    "    system_prompt = (\n",
    "        f\"You are a friendly and concise chatbot aiding in disaster response and GIS mapping, offering critical information, safety guidance, and spatial analysis for emergency operations. \"\n",
    "        f\"Here's the disaster report: \\n\"\n",
    "        f\"Disaster Type - {disaster_type}, \\n\"\n",
    "        f\"Disaster Location - {city}, {region}, {country} ({disaster_lat}, {disaster_lon}), \\n\"\n",
    "        f\"Casualties - {fatalities} fatalities, {injured} injured, {displaced} displaced, \\n\"\n",
    "        f\"user location:- {user_loc_str} \\n\"\n",
    "        f\"disaster image url: {disaster_img}\"\n",
    "        f\"if the user what image of the disaster give it to him in the following format [Disaster_Image](URL) eg. [Disaster_Image](https://images.axios.com/-0E5vD5wfUhUCvjCozznsl4ZJFw=/2023/09/11/1694467770528.jpg)\"\n",
    "    )\n",
    "    \n",
    "\n",
    "    return system_prompt\n",
    "\n",
    "user_location = {'latitude': 37.7749, 'longitude': -122.4194}\n",
    "user_location = \"San Francisco, CA\"\n",
    "system_prompt = create_system_prompt(disaster_info, user_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16858024-0476-4156-985c-e5050ee93ee8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:35:25.576735Z",
     "iopub.status.busy": "2024-12-07T18:35:25.576412Z",
     "iopub.status.idle": "2024-12-07T18:35:25.582173Z",
     "shell.execute_reply": "2024-12-07T18:35:25.581268Z",
     "shell.execute_reply.started": "2024-12-07T18:35:25.576709Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are a friendly and concise chatbot aiding in disaster response and GIS mapping, offering critical information, safety guidance, and spatial analysis for emergency operations. Here's the disaster report: \\nDisaster Type - earthquake, \\nDisaster Location - Los Angeles, California, United States (34.0522, -118.2437), \\nCasualties - 87 fatalities, 350 injured, 5600 displaced, \\nuser location:- User Location - San Francisco, CA \\ndisaster image url: https://images.axios.com/-0E5vD5wfUhUCvjCozznsl4ZJFw=/2023/09/11/1694467770528.jpgif the user what image of the disaster give it to him in the following format [Disaster_Image](URL) eg. [Disaster_Image](https://images.axios.com/-0E5vD5wfUhUCvjCozznsl4ZJFw=/2023/09/11/1694467770528.jpg)\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "280e2f8b-982e-49fc-96fe-c504eca58fc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:35:28.987353Z",
     "iopub.status.busy": "2024-12-07T18:35:28.986653Z",
     "iopub.status.idle": "2024-12-07T18:35:28.991925Z",
     "shell.execute_reply": "2024-12-07T18:35:28.990989Z",
     "shell.execute_reply.started": "2024-12-07T18:35:28.987320Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_response(text):\n",
    "    pattern = r'\\[Disaster_Image\\]\\((.*?)\\)'\n",
    "    url= re.findall(pattern, text[\"response\"])\n",
    "    if url:\n",
    "        text=text[\"response\"].replace(f\"[Disaster_Image]({url[0]})\",\"\")\n",
    "        url=url[0]\n",
    "    return {\"response\":text ,\"Image_url\": url if url else None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ed75839-d65c-4eac-b88d-c250420bbc90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:53:29.731458Z",
     "iopub.status.busy": "2024-12-07T18:53:29.731088Z",
     "iopub.status.idle": "2024-12-07T18:53:29.748831Z",
     "shell.execute_reply": "2024-12-07T18:53:29.747921Z",
     "shell.execute_reply.started": "2024-12-07T18:53:29.731427Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Conversation memory management\n",
    "class ConversationMemory:\n",
    "    def __init__(self):\n",
    "        self.memory = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt}\n",
    "        ]\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        self.memory.append({\"role\": role, \"content\": content})\n",
    "        return self.memory\n",
    "\n",
    "    def get_memory(self):\n",
    "        return self.memory\n",
    "\n",
    "# Initialize conversation memory\n",
    "conversation_memory = ConversationMemory()\n",
    "\n",
    "# Pydantic model for request validation\n",
    "class UserPrompt(BaseModel):\n",
    "    prompt: str\n",
    "\n",
    "def format_response(text):\n",
    "    pattern = r'\\[Disaster_Image\\]\\((.*?)\\)'\n",
    "    url = re.findall(pattern, text[\"response\"])\n",
    "    if url:\n",
    "        text[\"response\"] = text[\"response\"].replace(f\"[Disaster_Image]({url[0]})\", \"\")\n",
    "        url = url[0]\n",
    "    return {\"response\": text[\"response\"], \"Image_url\": url if url else None}\n",
    "\n",
    "def process_chat(user_input: UserPrompt):\n",
    "    try:\n",
    "        # Update conversation memory with user prompt\n",
    "        current_memory = conversation_memory.add_message(\"user\", user_input.prompt)\n",
    "        \n",
    "        # Prepare inputs for the model\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            current_memory,\n",
    "            tools=tools,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            add_generation_prompt=True,\n",
    "            return_dict=True,\n",
    "            repetition_penalty=1.1,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Move inputs to the correct device\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate response\n",
    "        outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "        response_text = tokenizer.decode(outputs[0][len(inputs[\"input_ids\"][0]):], skip_special_tokens=True)\n",
    "\n",
    "        # Check if the response contains a function call\n",
    "        if re.search(r'\\[{\"name\": \"(.*?)\", \"arguments\": (.*?)\\}]', response_text):\n",
    "            # Step 3: Generate a unique tool call ID\n",
    "            tool_call_id = ''.join(random.choices(string.ascii_letters + string.digits, k=9))\n",
    "\n",
    "            # Step 4: Parse response in JSON format\n",
    "            try:\n",
    "                tool_call = json.loads(response_text)[0]\n",
    "            except:\n",
    "                json_part = re.search(r'\\[.*\\]', response_text, re.DOTALL).group(0)\n",
    "                tool_call = json.loads(json_part)[0]\n",
    "\n",
    "            # Step 5: Executing Functions and Obtaining Results\n",
    "            function_name = tool_call[\"name\"]\n",
    "            arguments = tool_call[\"arguments\"]\n",
    "            tool_output = function_map[function_name](**arguments)\n",
    "\n",
    "            # Append tool call and tool response to messages\n",
    "            messages = current_memory.copy()\n",
    "            messages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"id\": tool_call_id, \"function\": tool_call}]})\n",
    "            messages.append({\"role\": \"tool\", \"tool_call_id\": tool_call_id, \"name\": function_name, \"content\": str(tool_output)})\n",
    "\n",
    "            # Step 6: Generating the Final Answer Based on Function Output\n",
    "            final_inputs = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=True,\n",
    "                return_dict=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            final_inputs = {k: v.to(model.device) for k, v in final_inputs.items()}\n",
    "            final_outputs = model.generate(**final_inputs, max_new_tokens=128)\n",
    "            final_response = tokenizer.decode(final_outputs[0][len(final_inputs[\"input_ids\"][0]):], skip_special_tokens=True)\n",
    "\n",
    "            # Update conversation memory with the final response\n",
    "            conversation_memory.add_message(\"assistant\", final_response)\n",
    "\n",
    "            return format_response({\n",
    "                \"response\": final_response,\n",
    "                \"memory_length\": len(conversation_memory.get_memory())\n",
    "            })\n",
    "        elif \"[Disaster_Image]\" in response_text:\n",
    "            temp_memory = [{\"role\": \"system\", \"content\": \"You are a friendly and concise chatbot.\"}]\n",
    "            temp_memory.append({\"role\": \"user\", \"content\": f\"Answer to message and give the following like it's the wanted image that answers the question {user_input.prompt}: {response_text}\"})\n",
    "            \n",
    "            # Prepare inputs for reformulation with the updated temporary memory\n",
    "            reformulation_inputs = tokenizer.apply_chat_template(\n",
    "                temp_memory,\n",
    "                tools=tools,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                add_generation_prompt=True,\n",
    "                return_dict=True,\n",
    "                repetition_penalty=1.1,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Move inputs to the correct device\n",
    "            reformulation_inputs = {k: v.to(model.device) for k, v in reformulation_inputs.items()}\n",
    "            \n",
    "            # Generate user-friendly response\n",
    "            reformulation_outputs = model.generate(**reformulation_inputs, max_new_tokens=200)\n",
    "            reformulation_context = tokenizer.decode(reformulation_inputs['input_ids'][0], skip_special_tokens=True)\n",
    "            full_reformulation = tokenizer.decode(reformulation_outputs[0], skip_special_tokens=True)\n",
    "            response_text = full_reformulation[len(reformulation_context):].strip()\n",
    "            conversation_memory.add_message(\"assistant\", response_text)\n",
    "            \n",
    "            return format_response({\n",
    "                \"response\": response_text,\n",
    "                \"memory_length\": len(conversation_memory.get_memory())\n",
    "            })\n",
    "    \n",
    "        else:\n",
    "            # If no function call, just add the assistant's response to memory\n",
    "            conversation_memory.add_message(\"assistant\", response_text)\n",
    "\n",
    "            return format_response({\n",
    "                \"response\": response_text,\n",
    "                \"memory_length\": len(conversation_memory.get_memory())\n",
    "            })\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "def memory():\n",
    "    return {\"response\": conversation_memory.get_memory()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36bd8107-2f88-4ca3-8bda-9d87d6a9a787",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:53:29.751253Z",
     "iopub.status.busy": "2024-12-07T18:53:29.750491Z",
     "iopub.status.idle": "2024-12-07T18:54:41.144530Z",
     "shell.execute_reply": "2024-12-07T18:54:41.143790Z",
     "shell.execute_reply.started": "2024-12-07T18:53:29.751224Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "user_input = UserPrompt(prompt=\"give me disaster image\")\n",
    "response = process_chat(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e137a30-41c3-4586-a8ab-d9be885bfbc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:54:41.146624Z",
     "iopub.status.busy": "2024-12-07T18:54:41.146070Z",
     "iopub.status.idle": "2024-12-07T18:54:41.152420Z",
     "shell.execute_reply": "2024-12-07T18:54:41.151607Z",
     "shell.execute_reply.started": "2024-12-07T18:54:41.146584Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': 'Here is a disaster image: ',\n",
       " 'Image_url': 'https://images.axios.com/-0E5vD5wfUhUCvjCozznsl4ZJFw=/2023/09/11/1694467770528.jpg'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c681b-4f2a-4d9d-8315-ae81b51bb8ba",
   "metadata": {},
   "source": [
    "# Setting API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd18b6-2812-402c-b108-fa7b3fa86045",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import ngrok\n",
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "import ngrok\n",
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1380a46-9044-4106-9310-44f6e33d4514",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "\n",
    "app = FastAPI(title=\"Conversational AI Chatbot\")\n",
    "\n",
    "# Conversation memory management\n",
    "class ConversationMemory:\n",
    "    def __init__(self, max_memory=10):\n",
    "        self.memory = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt}\n",
    "        ]\n",
    "\n",
    "        self.max_memory = max_memory\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        # Add new message to memory\n",
    "        self.memory.append({\"role\": role, \"content\": content})\n",
    "        \n",
    "        # Trim memory if it exceeds max length\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            self.memory = self.memory[-self.max_memory:]\n",
    "        \n",
    "        return self.memory\n",
    "\n",
    "    def get_memory(self):\n",
    "        return self.memory\n",
    "\n",
    "# Initialize conversation memory\n",
    "conversation_memory = ConversationMemory()\n",
    "\n",
    "# Pydantic model for request validation\n",
    "class UserPrompt(BaseModel):\n",
    "    prompt: str\n",
    "\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "def process_chat(user_input: UserPrompt):\n",
    "    try:\n",
    "        # Update conversation memory with user prompt\n",
    "        current_memory = conversation_memory.add_message(\"user\", user_input.prompt)\n",
    "        \n",
    "        # Prepare inputs for the model\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            current_memory,\n",
    "            tools=tools,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            add_generation_prompt=True,\n",
    "            return_dict=True,\n",
    "            repetition_penalty=1.1,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "\n",
    "        # Move inputs to the correct device\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate response\n",
    "        outputs = model.generate(**inputs, max_new_tokens=1000)\n",
    "\n",
    "        input_context = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
    "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response_text = full_response[len(input_context):].strip()\n",
    "\n",
    "        match = re.search(r'\\[{\"name\": \"(.*?)\", \"arguments\": (.*?)\\}]', response_text)\n",
    "        if match:\n",
    "            # Execute the function (which will always return a dictionary)\n",
    "            function_result = exec_called_func(response_text, function_map)\n",
    "            \n",
    "            # Convert dictionary to a string description\n",
    "            function_result_str = \" \".join([f\"{k}: {v}\" for k, v in function_result.items()])\n",
    "            \n",
    "            \n",
    "            # Create a copy of the current conversation memory\n",
    "            temp_memory = [{\"role\": \"system\", \"content\": \"You are a friendly and concise chatbot.\"}]\n",
    "            temp_memory.append({\"role\": \"user\", \"content\": f\"Here are the details: {function_result_str}.give conversational response about this information.that answer the question {user_input.prompt}\"})\n",
    "            \n",
    "            # Prepare inputs for reformulation with the updated temporary memory\n",
    "            reformulation_inputs = tokenizer.apply_chat_template(\n",
    "            temp_memory,\n",
    "            tools=tools,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            add_generation_prompt=True,\n",
    "            return_dict=True,\n",
    "            repetition_penalty=1.1,\n",
    "            return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Move inputs to the correct device\n",
    "            reformulation_inputs = {k: v.to(model.device) for k, v in reformulation_inputs.items()}\n",
    "            \n",
    "            # Generate user-friendly response\n",
    "            reformulation_outputs = model.generate(**reformulation_inputs, max_new_tokens=200)\n",
    "            reformulation_context = tokenizer.decode(reformulation_inputs['input_ids'][0], skip_special_tokens=True)\n",
    "            full_reformulation = tokenizer.decode(reformulation_outputs[0], skip_special_tokens=True)\n",
    "            response_text = full_reformulation[len(reformulation_context):].strip()\n",
    "            \n",
    "            # Update memory with the final reformulated response\n",
    "            conversation_memory.add_message(\"assistant\", response_text)\n",
    "        elif \"[Disaster_Image]\" in response_text:\n",
    "            temp_memory = [{\"role\": \"system\", \"content\": \"You are a friendly and concise chatbot.\"}]\n",
    "            temp_memory.append({\"role\": \"user\", \"content\": f\"Answer to message and  give the following like it's the wanted image that answers the question {user_input.prompt}: {response_text}\"})\n",
    "            \n",
    "            # Prepare inputs for reformulation with the updated temporary memory\n",
    "            reformulation_inputs = tokenizer.apply_chat_template(\n",
    "            temp_memory,\n",
    "            tools=tools,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            add_generation_prompt=True,\n",
    "            return_dict=True,\n",
    "            repetition_penalty=1.1,\n",
    "            return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Move inputs to the correct device\n",
    "            reformulation_inputs = {k: v.to(model.device) for k, v in reformulation_inputs.items()}\n",
    "            \n",
    "            # Generate user-friendly response\n",
    "            reformulation_outputs = model.generate(**reformulation_inputs, max_new_tokens=200)\n",
    "            reformulation_context = tokenizer.decode(reformulation_inputs['input_ids'][0], skip_special_tokens=True)\n",
    "            full_reformulation = tokenizer.decode(reformulation_outputs[0], skip_special_tokens=True)\n",
    "            response_text = full_reformulation[len(reformulation_context):].strip()\n",
    "            conversation_memory.add_message(\"assistant\", response_text)\n",
    "        else:\n",
    "            # If no function call, just add the assistant's response to memory\n",
    "            conversation_memory.add_message(\"assistant\", response_text)\n",
    "        \n",
    "        \n",
    "        return format_response({\n",
    "            \"response\":response_text,\n",
    "            \"memory_length\": len(conversation_memory.get_memory())\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/memory\")\n",
    "def memory():\n",
    "    return {\"response\":conversation_memory.get_memory()}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b0e6da-b9d8-4969-bf11-3cbe84d2f069",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419aede0-338e-46f6-86a7-b209a5ed1eb0",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # Replace \"\" with a specific domain, e.g., [\"http://localhost:5173\"]\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],  # Allow all HTTP methods\n",
    "    allow_headers=[\"*\"],  # Allow all headers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81276c0e-e9bb-4899-a990-35aab7daa3a9",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#ngrok.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c1996-0b6e-49a3-a945-eb84773dd821",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8781e9ea-8b12-41a7-9524-aad806757c5d",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#listener = ngrok.forward(8000,authtoken=\"2paEia7f6PSBy9lNLz5KUDmTayS_7r2SJK1FvRPSpuBnmiE7x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3281e202-eece-4958-9c57-c2d20a35a989",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tunnel = await listener\n",
    "print(f\"Ingress established at {tunnel.url()}\")\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app, port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6de78d9-7963-45b0-8fbd-ed2d66346ec3",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5631016,
     "sourceId": 9300112,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 210556512,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.734316,
   "end_time": "2024-11-30T12:59:34.374568",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-30T12:59:29.640252",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
